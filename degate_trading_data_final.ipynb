{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ba990c-75cf-480b-94fb-be482c5ba2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests import Request\n",
    "import json\n",
    "import csv\n",
    "import dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from flipside import Flipside\n",
    "import logging\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb961bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dotenv.load_dotenv('../../.env')\n",
    "api_key = os.environ[\"DUNE_API_KEY\"]\n",
    "headers = {\"X-Dune-API-Key\": api_key}\n",
    "flipside_key = os.environ[\"FLIPSIDE_API_KEY\"]\n",
    "output_dir = \"data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4275bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#  TOKEN METADATA\n",
    "# ==========================\n",
    "\n",
    "def fetch_token_metadata():\n",
    "    \"\"\"\n",
    "    Fetches the list of tokens registered on the platform from Flipside API.\n",
    "    \n",
    "    :param flipside_key: API key for FlipsideCrypto\n",
    "    :param contract_address: The contract address to filter registered tokens\n",
    "    :return: List of registered tokens\n",
    "    \"\"\"\n",
    "    try:\n",
    "        flipside = Flipside(flipside_key, \"https://api-v2.flipsidecrypto.xyz\")\n",
    "    \n",
    "        sql = \"\"\"with tokens as (\n",
    "        SELECT \n",
    "        DECODED_LOG: token as token\n",
    "        , DECODED_LOG: tokenId as tokenId\n",
    "        , *\n",
    "        FROM ethereum.core.ez_decoded_event_logs\n",
    "\n",
    "        where CONTRACT_ADDRESS = '0x9c07a72177c5a05410ca338823e790876e79d73b'\n",
    "        AND EVENT_NAME = 'TokenRegistered'\n",
    "        )\n",
    "\n",
    "        SELECT \n",
    "        DISTINCT token\n",
    "        , tokenId\n",
    "        , case \n",
    "            when token = '0x0000000000000000000000000000000000000000' then 'ETH'\n",
    "            when token = '0xf2d4900994b24adac8b396500f24e0557e2bf84d' then 'RATS'\n",
    "            when token = '0x32e4a492068bee178a42382699dbbe8eef078800' then 'BIRDPAW'\n",
    "            when token = '0x4877d83faac234dbd38d0f17bbc27a80604d3aed' then 'Illiterate' --DeflationaryToken\n",
    "            when token = '0x3f1ee2f15da3eaf3539006b651144ec87755876d' then 'BRETT' --deGate -- Brdg Token\n",
    "            when token = '0x4b63ce7e179d1db5ddac5d9d54e48356cf3e8b7d' then 'AIB' -- on ERC20 V1 (AIB)\n",
    "            when token = '0xa1cfa45c8313c2da73b93454adcb1dab24f0993a' then 'Elon' --Eloncoin \n",
    "            when token = '0xe97fb0268474ef444d732f4ce5cfa6d8772b97c6' then 'W' --Wormhole token \n",
    "            when token = '0x9aea32b459e96c8ef5010f69130bf95fd129ac05' then 'WKLAY' --Wrapped Klay  Wormhole -BT\n",
    "            when token = '0x754e822b92ba9be681c461347e0d2d38abc0cf16' then 'LBS' --Lbscoin\n",
    "            when token = '0xe31cfcd36fed044ae4cf9405b577fe875762194f' then 'ALGO'\n",
    "            when token = '0xbf8c53c59fad2aff7ffd925db435ea10c5ea4b6c' then 'SIC' --Sicuro \n",
    "            when token = '0x061f60d153beeb3f78ed5f38bb326c5b20a65503' then 'ZPG' --zk pet dog \n",
    "            when token = '0x8cdf7af57e4c8b930e1b23c477c22f076530585e' then 'APT' --Aptos Coin (\n",
    "            when token = '0x1c88d38d04acd3edd9051ec587c67abff04bf30d' then 'NEAR' --Bridget Token contract (BT)\n",
    "            when token = '0x215cff9fa9f3466b07bc6b5a5f30b925fb71163b' then 'MR' -- Martin \n",
    "            when token = '0xccf27d3fff920d999cc7e8a3fc847a96bca44ccd' then 'SVR' -- SVR Token \n",
    "            when token = '0x366863c4d67f87cc1238d1008c7053f96e53e559' then 'Sailor'\n",
    "            when token = '0x99613a517d20944246ab6ef124a735227a8c1af3' then 'Olda' --Oldacoin\n",
    "            when token = '0xce0cd513a069e8ec9cb625fcdf6d5f29aa912dbc' then 'MMS'\n",
    "            when token = '0x3635ffc4f860055a7f64365d39a27de3d84eb78b' then 'MAGA' --Magacoin (\n",
    "            when token = '0x06b089cbf0403ac2c5f452584f8a18978019b858' then 'TBC' --Trade Bot Coin (\n",
    "            when token = '0xb821b75b42da3c9f38383e457fa33c4e4b85a314' then 'GREN' -- Grencoin (\n",
    "            when token = '0x6215a0fc6ba68cbb0f99a9e1d3a5adf1321b6eb7' then '$MUSK' --MUSK COIN \n",
    "            when token = '0xefc0ced4b3d536103e76a1c4c74f0385c8f4bdd3' then 'PYTH'--Pyth Network \n",
    "            when token = '0x84074ea631dec7a4edcd5303d164d5dea4c653d6' then 'SUI' --Bridget Token contract\n",
    "            when token = '0xf91e605af079384cc7077b3914a4a36019a89ee8' then 'SEI' --Bridget Token contract\n",
    "            when token = '0xf4feec8cf825cd5b23f8abb3075c01c22abd4352' then 'DEGEN' -- BT\n",
    "            when token = '0x8a00bf67a9bb032204da83408c4d1cd5421b40b8' then 'DEGEN' --DEGEN Donkey \n",
    "            when token = '0x57fbf85655c3a08bffe37a4f32f8adbd369508df' then 'OLD' --Oldtown Coin (\n",
    "            when token = '0x8f5affe2443ea12c575ad5b13bf8fd235ed184c9' then 'TEST' \n",
    "            when token = '0x4b63ce7e179d1db5ddac5d9d54e48356cf3e8b7d' then 'AIB' \n",
    "            when token = '0x256a63a4900bddcf7703b601ac0b70aa2d7f9318' then 'SORA'\n",
    "            when token = '0xf852ffa34a20113cd741f3cd9406a1a86b70c8ab' then 'UNI-V2' -- SAGE/ETH pair\n",
    "            when token = '0xf02123509a08632339102ee5fdd41b638592e495' then 'VEN' --DUCATO \n",
    "            when token = '0xf2fdd72bd1581b9bca7b4391975dbacca7ec37e8' then 'SHIB' ---SHIBcoin \n",
    "            when token = '0x2ed58b1fa208e9a08fdaac2a839b8539abe558e8' then 'WIF' --BT - dogwifhat \n",
    "            when token = '0x5e9b0c790707b95457d56fc8c6411662f61d4d98' then 'LYV' -- LYV Finance (\n",
    "            when token = '0xe32e3851e0a4216581342defac353d1efdfb36d9' then 'SMFC' --Social Media Finance Coin (\n",
    "            when token = '0x285308b5fc68cc0f737c77cb60042b1fb5633e81' then 'ZEUS' --Zeus Vip Coin \n",
    "            when token = '0x8ce949b02edc782c04f9c618396a9f8c0e2b9274' then 'ZBC' -- Zeus Basis Coin \n",
    "            when token = '0x1df721d242e0783f8fcab4a9ffe4f35bdf329909' then 'OP' -- BT Optimism \n",
    "            when token = '0x8687a10bca6f139b25eb31020fcabb5782214764' then 'JUP' --Jupiter \n",
    "            when token = '0x0a866a8256832aaf048f274b9b538e795b64137f' then 'NAP' --Napcoin \n",
    "            else symbol end as symbol\n",
    "        FROM tokens t\n",
    "        left join ethereum.price.ez_asset_metadata p ON t.token = p.token_address\n",
    "        \"\"\"\n",
    "\n",
    "        query_result_set = flipside.query(sql)\n",
    "\n",
    "        if not query_result_set or not hasattr(query_result_set, 'query_id'):\n",
    "                logging.error(\"Failed to get valid query result set from Flipside\")\n",
    "                return None\n",
    "        \n",
    "        logging.info(f\"Query submitted successfully with ID: {query_result_set.query_id}\")\n",
    "            \n",
    "\n",
    "        all_records = auto_paginate_result(query_result_set)\n",
    "        if not all_records:\n",
    "                logging.warning(\"No records returned from query\")\n",
    "                return None\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_records)\n",
    "        if df.empty:\n",
    "                logging.warning(\"DataFrame is empty after conversion\")\n",
    "                return None\n",
    "        \n",
    "        # Generate timestamp\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = \"data\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "        filename = os.path.join(output_dir, f\"token_metadata_{timestamp}.csv\")\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        logging.info(f\"Token metadata saved to {filename} with {len(df)} records\")\n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in fetch_token_metadata: {e}\")\n",
    "\n",
    "def auto_paginate_result(query_result_set, page_size=100):\n",
    "    \"\"\"\n",
    "    Automatically paginates through the API results and returns all records.\n",
    "    \"\"\"\n",
    "    flipside = Flipside(flipside_key, \"https://api-v2.flipsidecrypto.xyz\")\n",
    "    current_page_number = 1\n",
    "    total_pages = 2  # Initial assumption until we get actual total pages\n",
    "    all_records = []\n",
    "    \n",
    "    while current_page_number <= total_pages:\n",
    "        try:\n",
    "            results = flipside.get_query_results(\n",
    "                query_result_set.query_id,\n",
    "                page_number=current_page_number,\n",
    "                page_size=page_size\n",
    "            )\n",
    "\n",
    "            # Update total pages from the response\n",
    "            total_pages = results.page.totalPages\n",
    "            \n",
    "            # Append records if available\n",
    "            if results.records:\n",
    "                all_records.extend(results.records)\n",
    "                logging.info(f\"Fetched page {current_page_number}/{total_pages}\")\n",
    "                current_page_number += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {current_page_number}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3d3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#  VALID TRADING PAIRS\n",
    "# ==========================\n",
    "\n",
    "QUOTE_CURRENCY_PAIRS = {\n",
    "    \"USDT\": \"all\",\n",
    "    \"USDC\": \"all\",\n",
    "    \"WBTC\": [\"ETH\", \"SOL\"],\n",
    "    \"ETH\": \"all\",\n",
    "    \"USDM\": [\"ETH\", \"DG\", \"MAP\", \"wsETH\", \"WBTC\", \"GRT\"],\n",
    "    \"LUSD\": [\"ETH\"]\n",
    "}\n",
    "BASE_URL = \"https://v1-mainnet-backend.degate.com/order-book-ws-api/ticker/bookTicker\"\n",
    "\n",
    "# Session for API requests\n",
    "session = requests.Session()\n",
    "\n",
    "def fetch_ticker(base_symbol, base_id, quote_symbol, quote_id, token_dict, QUOTE_CURRENCY_PAIRS):\n",
    "    \"\"\"\n",
    "    Fetch ticker data for a given base-quote pair and return structured data if valid.\n",
    "    \"\"\"\n",
    "    pair_name = f\"{base_symbol}/{quote_symbol}\"\n",
    "    params = {\"base_token_id\": base_id, \"quote_token_id\": quote_id}\n",
    "\n",
    "    try:\n",
    "        response = session.get(BASE_URL, params=params, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        # Validate API response\n",
    "        if not isinstance(result, dict) or \"code\" not in result:\n",
    "            logging.error(f\"Unexpected response format for {pair_name}: {result}\")\n",
    "            return None\n",
    "\n",
    "        if result.get(\"code\") == 0:\n",
    "            logging.info(f\"Valid trading pair found: {pair_name}\")\n",
    "            return {\n",
    "                \"pair\": pair_name,\n",
    "                \"base_symbol\": base_symbol,\n",
    "                \"base_id\": base_id,\n",
    "                \"quote_symbol\": quote_symbol,\n",
    "                \"quote_id\": quote_id\n",
    "            }\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error fetching data for {pair_name}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_valid_pairs(new_tokens=None):\n",
    "    \"\"\"\n",
    "    Main function to fetch and validate trading pairs.\n",
    "    If `new_tokens` is passed, only those tokens are processed.\n",
    "    \"\"\"\n",
    "    valid_trading_pairs = []\n",
    "     # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load token list and filter out test tokens\n",
    "    token_metadata_path = os.path.join(output_dir, \"token_metadata.csv\")\n",
    "    \n",
    "    # Load token list and filter out test tokens\n",
    "    try: \n",
    "        df = pd.read_csv(token_metadata_path).rename(columns={'tokenid': 'id'}).drop(columns=['__row_index'], errors='ignore')\n",
    "        logging.info(f\"Token metadata loaded from {token_metadata_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File '{token_metadata_path}' not found. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Create a dictionary for token lookup (symbol -> id)\n",
    "    token_dict = df.set_index(\"symbol\")[\"id\"].to_dict()\n",
    "\n",
    "    # Filter out test tokens early to avoid unnecessary processing\n",
    "    filtered_tokens = {symbol: token_id for symbol, token_id in token_dict.items() if \"TEST\" not in symbol}\n",
    "\n",
    "     # If new_tokens are provided, only process those\n",
    "    if new_tokens:\n",
    "        filtered_tokens = {symbol: token_id for symbol, token_id in filtered_tokens.items() if symbol in new_tokens}\n",
    "    \n",
    "   \n",
    "    # Prepare a list of valid quote tokens to avoid unnecessary iterations\n",
    "    valid_quote_tokens = {q for q in QUOTE_CURRENCY_PAIRS.keys() if q in token_dict}\n",
    "\n",
    "\n",
    "    # ThreadPoolExecutor with dynamic task submission\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "\n",
    "        for base_symbol, base_id in filtered_tokens.items():\n",
    "            for quote_symbol in valid_quote_tokens:\n",
    "                valid_bases = QUOTE_CURRENCY_PAIRS[quote_symbol]\n",
    "                \n",
    "                # Skip if quote symbol doesn't allow trading with base\n",
    "                if valid_bases != \"all\" and base_symbol not in valid_bases:\n",
    "                    continue\n",
    "\n",
    "                quote_id = token_dict[quote_symbol]\n",
    "                futures.append(executor.submit(fetch_ticker, base_symbol, base_id, quote_symbol, quote_id, token_dict, QUOTE_CURRENCY_PAIRS))\n",
    "\n",
    "                # Prevent excessive task queuing\n",
    "                if len(futures) >= 500:  # Process tasks in batches (500 at a time) to prevent memory overload \n",
    "                    for future in as_completed(futures): # as_completed returns futures in order of completion\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            valid_trading_pairs.append(result)\n",
    "                    futures.clear() # free up memory before the next batch\n",
    "\n",
    "        # Process remaining tasks\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                valid_trading_pairs.append(result)\n",
    "\n",
    "\n",
    "    # Save valid pairs to CSV with 5 columns\n",
    "    if valid_trading_pairs:\n",
    "        output_path = os.path.join(output_dir, \"valid_trading_pairs.csv\")\n",
    "        df_valid_pairs = pd.DataFrame(valid_trading_pairs)\n",
    "        df_valid_pairs.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Valid trading pairs saved successfully to {output_path}.\")\n",
    "    else:\n",
    "        logging.warning(\"No valid trading pairs found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac1dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#  FETCH TRADING DATA\n",
    "# ==========================\n",
    "\n",
    "# takes the output file as input from dg3_fetch_klines main() and returns the latest close time from the file\n",
    "def get_latest_close_time():\n",
    "    try:\n",
    "        files = sorted(glob.glob(os.path.join(output_dir, 'trading_data_*.csv')))\n",
    "        if not files:\n",
    "            logging.warning(\"No trading data files found. Assuming first run.\")\n",
    "            return None\n",
    "        \n",
    "        last_trading_data_file = files[-1]\n",
    "        df = pd.read_csv(last_trading_data_file)\n",
    "        \n",
    "        max_close_time = df.iloc[:, 7].max()  # Assuming the 8th column contains close_time\n",
    "        if pd.isna(max_close_time):\n",
    "            logging.warning(f\"No valid 'close_time' found in {last_trading_data_file}. Assuming first run.\")\n",
    "            return None\n",
    "        \n",
    "        logging.info(f\"Latest close_time found: {max_close_time}\")\n",
    "        return int(max_close_time)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading trading data file: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# generates the startend time for the klines fetch\n",
    "def get_end_time_calculation():\n",
    "    today_utc = datetime.now()  # Ensure UTC time\n",
    "    return int(today_utc.timestamp()) * 1000\n",
    "\n",
    "\n",
    "BASE_URL_KLINES = \"https://v1-mainnet-backend.degate.com/order-book-ws-api/klines\"\n",
    "GRANULARITY = 86400  \n",
    "session = requests.Session()\n",
    "    # pairs_df = pd.read_csv(\"valid_trading_pairs.csv\")\n",
    "def fetch_trading_data_by_pair(pair, base_id, quote_id, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Fetch trading data for a given pair using session.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"base_token_id\": base_id,\n",
    "        \"quote_token_id\": quote_id,\n",
    "        \"granularity\": GRANULARITY,\n",
    "        \"start\": start_time, #START_TIME,\n",
    "        \"end\": end_time #END_TIME\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Fetching data for {pair} from {start_time} to {end_time}\")\n",
    "        response = session.get(BASE_URL_KLINES, params=params, timeout=3)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Log API response for debugging\n",
    "        if \"data\" not in result:\n",
    "            logging.warning(f\"No 'data' key in API response for {pair}: {result}\")\n",
    "            return None\n",
    "\n",
    "        data = result.get(\"data\", [])\n",
    "\n",
    "        if not data:\n",
    "            logging.info(f\"No trading data for {pair} in the requested period.\")\n",
    "            return None\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.insert(0, \"pair\", pair)\n",
    "        return df\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error fetching {pair}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_all_trading_data(start_time=None, end_time=None, output_file=None):    \n",
    "    all_data = []\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load token list and filter out test tokens\n",
    "    valid_pairs_path = os.path.join(output_dir, \"valid_trading_pairs.csv\")\n",
    "    \n",
    "    # Load token list and filter out test tokens\n",
    "    try: \n",
    "        pairs_df = pd.read_csv(valid_pairs_path)\n",
    "        logging.info(f\"Valid pairs loaded from {valid_pairs_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File '{valid_pairs_path}' not found. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # allows multiple API requests to run concurrently, improving performance\n",
    "    with ThreadPoolExecutor(max_workers=min(10, len(pairs_df))) as executor:\n",
    "        \n",
    "        # stores async results as a key and tradig pair as a value\n",
    "        futures = { \n",
    "            # submits API call for each pair\n",
    "            executor.submit(fetch_trading_data_by_pair, row[\"pair\"], row[\"base_id\"], row[\"quote_id\"], start_time, end_time): row[\"pair\"]\n",
    "            for _, row in pairs_df.iterrows()\n",
    "            }\n",
    "        # collecting results form API calls\n",
    "        for future in as_completed(futures):\n",
    "            # gets the returned data and add it to all_data if not None\n",
    "            if (result := future.result()) is not None:\n",
    "                all_data.append(result)\n",
    "   \n",
    "   # Saving the data to a CSV file\n",
    "    if all_data:\n",
    "        # Generate a unique filename if none is provided\n",
    "        if output_file is None:\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")  # Example: 20250228_165430\n",
    "            output_file = f\"trading_data_{timestamp}.csv\"\n",
    "\n",
    "        # Combine output_dir and output_file\n",
    "        output_file_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "        pd.concat(all_data, ignore_index=True).to_csv(output_file_path, index=False) #test_raw_trading_data.csv\n",
    "        print(f\"INFO: Saved trading data to {output_file_path}.\")\n",
    "\n",
    "        # Pass the output file to the next function for further processing\n",
    "        clean_and_prepare_data(output_file_path)\n",
    "    \n",
    "        return output_file_path\n",
    "    \n",
    "    else:\n",
    "        logging.warning(\"No trading data retrieved.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60507c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#  FORMATTING & UPLOADING\n",
    "# ==========================\n",
    "\n",
    "import time\n",
    "def clean_and_prepare_data(input_file):\n",
    "    \"\"\"\n",
    "    Clean and prepare the trading data from the input file.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        logging.info(f\"File {input_file} loaded successfully with shape {df.shape}\")\n",
    "\n",
    "        base = df.copy()\n",
    "        # logging.info(\"Dataframe copied for transformation.\")\n",
    "        base.rename(columns={'0':'date','1':'open','2':'high','3':'low','4':'close','5':'volume','6':'close_time',\n",
    "                            '7':'quote_volume','8':'trades','9':'taker_buy_base_vol','10':'taker_buy_quote_vol',\n",
    "                            '11':'ignore','12':'_avg_price'}, inplace=True)\n",
    "        logging.info(f\"Columns renamed successfully. New columns: {list(base.columns)}\")\n",
    "\n",
    "        base = base.drop(['taker_buy_base_vol','taker_buy_quote_vol','ignore','_avg_price'], axis=1)\n",
    "        logging.info(f\"Columns dropped successfully. New columns: {list(base.columns)}\")\n",
    "\n",
    "        # Prepare df for further uploads\n",
    "        base['date'] = pd.to_datetime(base['date'], unit='ms')\n",
    "        base['close_time'] = pd.to_datetime(base['close_time'], unit='ms')\n",
    "        logging.info(f\"Converted 'date' and 'close_time' to datetime format.\")\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the final cleaned data to the \"data\" directory with timestamp\n",
    "        cleaned_file_path = os.path.join(output_dir, f'degate_updates_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "\n",
    "        base.to_csv(cleaned_file_path, index=False)\n",
    "        logging.info(f\"Data cleaned and saved to {cleaned_file_path}\")\n",
    "\n",
    "        # Upload to Dune with the correct path\n",
    "        upload_data_to_dune(cleaned_file_path)\n",
    "        \n",
    "        return cleaned_file_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data cleaning process: {e}\", exc_info=True)\n",
    "        return None\n",
    "    \n",
    "\n",
    "def upload_data_to_dune(file_path):\n",
    "    \"\"\"\n",
    "    Upload the data to Dune.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://api.dune.com/api/v1/table/kemasan/degate_trades/insert\"\n",
    "        headers = {\n",
    "            \"X-DUNE-API-KEY\": api_key,\n",
    "            \"Content-Type\": \"text/csv\"\n",
    "        }\n",
    "        # last_trading_data_file = sorted(glob.glob('degate_updates__*.csv'))[-1]\n",
    "        with open(file_path, \"rb\") as data:\n",
    "            response = requests.request(\"POST\", url, data=data, headers=headers)\n",
    "        logging.info(f\"Data uploaded to Dune successfully with response: {response.text}\")\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data upload process: {e}\", exc_info=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#  PROCESS NEW TOKENS\n",
    "# ==========================\n",
    "\n",
    "def load_token_list(file_path):\n",
    "    \"\"\" Load token list from a given CSV file. \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # return set(df[\"symbol\"])\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"File {file_path} not found. Returning empty token list.\")\n",
    "        # return set()\n",
    "        return pd.DataFrame(columns=[\"symbol\", \"token\", \"tokenId\"])\n",
    "    \n",
    "def update_token_metadata(new_tokens, existing_token_df, new_token_data, file_path):\n",
    "    \"\"\" \n",
    "    Update token metadata with new tokens and save it to the CSV. \n",
    "    \"\"\"\n",
    "    if new_tokens:\n",
    "        logging.info(f\"New tokens detected: {new_tokens}\")\n",
    "        \n",
    "        # Filter new tokens from the incoming data\n",
    "        new_tokens_data = new_token_data[new_token_data[\"symbol\"].isin(new_tokens)]\n",
    "        \n",
    "        # Append new data to old data and save\n",
    "        updated_data = pd.concat([existing_token_df, new_tokens_data], ignore_index=True)\n",
    "        updated_data.to_csv(file_path, index=False)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "output_dir = \"data\"\n",
    "def process_new_tokens():\n",
    "    \"\"\" \n",
    "    Process and handle new tokens. \n",
    "    \"\"\"\n",
    "    # Load existing tokens from token_metadata.csv in output_dir\n",
    "    token_metadata_path = os.path.join(output_dir, \"token_metadata.csv\")\n",
    "    existing_token_df = load_token_list(token_metadata_path)\n",
    "    existing_token_symbols = set(existing_token_df[\"symbol\"])  \n",
    "    # Fetch the latest token list from Flipside API (simulated here)\n",
    "    try:\n",
    "        latest_token_file = sorted(glob.glob(os.path.join(output_dir,\"token_metadata_*.csv\")))[-1] # from fetch_token_metadata()\n",
    "        new_token_data = pd.read_csv(latest_token_file)\n",
    "        new_token_list = set(new_token_data[\"symbol\"])\n",
    "    except IndexError:\n",
    "        logging.error(\"No new token data found.\")\n",
    "        return\n",
    "    \n",
    "    new_tokens = new_token_list - existing_token_symbols\n",
    "    # new_tokens.to_csv(f\"new_tokens_{time.strftime('%Y%m%d_%H%M%S')}.csv\", index=False) # pass file to get_valid_pairs()\n",
    "    # If there are new tokens, update the metadata and fetch valid pairs\n",
    "    if new_tokens:\n",
    "        logging.info(f\"New tokens detected: {new_tokens}\")\n",
    "\n",
    "        if update_token_metadata(new_tokens, existing_token_df, new_token_data,token_metadata_path):\n",
    "            logging.info(\"Fetching valid pairs for new tokens...\")\n",
    "            \n",
    "            # call the function to get the valid pairs\n",
    "            get_valid_pairs(new_tokens) # pass here a new tokens to avoid the processing of all tokens\n",
    "            logging.info(\"Fetching trading data for new tokens...\")\n",
    "            \n",
    "            # call the function to fetch trading data\n",
    "            fetch_all_trading_data(get_latest_close_time() + 1, get_end_time_calculation())\n",
    "    else:\n",
    "        logging.info(\"No new tokens detected. Proceeding with trading data update.\")\n",
    "        fetch_all_trading_data(get_latest_close_time() + 1, get_end_time_calculation())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99454f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#  MAIN EXECUTION\n",
    "# ==========================\n",
    "def main():\n",
    "    \"\"\" Main execution entry point. \"\"\"\n",
    "    try:\n",
    "        process_new_tokens()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
